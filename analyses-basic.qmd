---
title: "Data cleaning, descriptives, reliability, histograms, correlations"
author: "TJ Sullivan"
format: html
keep-md: true 
embed-resources: true
---

```{r setup, include = F}
# sets warnings 
#knitr::opts_chunk$set(warning = FALSE) 
#rmarkdown::render("LGBTQ-APPS_analyses_RMarkdown_2022.03.30.Rmd", params = "ask")
```

# Load packages and data

```{r load packages, warning = F, message = F}
library(tidyverse)
library(sjmisc)
library(haven)
library(easystats)
library(purrr)
library(psych)
library(gridExtra)
library(lme4)
library(lmerTest)
```

Load apps dataset, which contains LADI coding information and relevant variables

```{r load apps data}
apps <- read_spss("data/LGBTQ-APPS 2.24.22 KB.sav")
```

Load full dataset to use for reporting demographics, calculating reliability, and pulling variables not included in prior apps dataset when initially requested.

```{r load full dataset}
full <- read_spss("data/ESTEEM All Data Merged_updated 3.2.21_with_comorbidity_RB.sav")
```

Reduce full dataset to the n=57 people who had coded data for this study.

```{r reduce full dataset}
id <- select(apps, ParticipantID)
full <- id |> left_join(full, by = "ParticipantID")
rm(id)
```

Merge variables not included in the apps dataset (this includes HIV risk transmission behavior from TLFB, AUDIT, SIDAS, and SIP-AD).

```{r merge other variables}
# isolate variables to merge
merge <- 
  full |> 
  select(ParticipantID, Assessment, Sx_CASriskacts, Sx_totalacts_sum, SIP_sum, AUDIT_sum, SIDAS_sum) |> 
  # change Assessment variable to character to match dataset; add in missing variables for BL 
  mutate(Assessment = case_when(Assessment == 0 ~ "BL",
                                Assessment == 1 ~ "4M",
                                Assessment == 2 ~ "8M",
                                Assessment == 3 ~ "12M")) |> 
  pivot_wider(id_cols = ParticipantID,
              names_from = Assessment,
              values_from = c(-ParticipantID, -Assessment),
              names_sep = ".")
# merge
apps <- left_join(apps, merge, by = "ParticipantID") |> 
  # create a variable for whether pt was engaged in CMHT, general MH services, or medication at BL - should be 0 for everyone
  mutate(CMHT.BL = 0, 
         MHgen.BL = 0,
         Medication.BL = 0)
rm(merge)
```

# Data restructuring

First we need to re-structure the dataset to long format. One dataframe will contain the outcome/follow-up assessment data, one will contain alliance data. This is done because there are differing numbers of time points - 4 assessments (BL, 4MFU, 8MFU, 12MFU) and 10 sessions.

Because we are interested in controlling for the therapy alliance in analyses, we need to separate out the alliance data and pull the alliance scores for the sessions where the LGBTQ-APPS was coded. This code will do that and merge it with the main dataset before restructuring.

```{r separate out alliance data p2}
apps <- mutate(apps, WAI_sum_missing = rowSums(is.na(across(WAI_P_Sum.Session1:WAI_P_Sum.Session10))),
         WAI_sum_missing = recode_factor(if_else(WAI_sum_missing > 0, 1, 0), `0` = "No missing data for any session", `1` = "Missing data for at least 1 session")) # create a variable denoting whether participant has complete WAI data or is missing at least one session

apps_alliance <-
  apps |> 
  select(ParticipantID, Early_Session, Mid_Session, WAI_P_Sum.Session1:WAI_P_Sum.Session10) |> 
  pivot_longer(cols = c(-ParticipantID, -Early_Session, -Mid_Session),
    names_to = c(".value", "Session"), 
    names_sep = "\\.Session") |> 
  # rename alliance variable 
  rename(WAI.Total = WAI_P_Sum)

early <- apps_alliance |> select(ParticipantID, Session, Early_Session, Mid_Session, WAI.Total) |> 
  # match the alliance score for the specific session that was coded 
  filter(if_else(Early_Session == 3, Session == 3, Session == 2)) |> 
  select(ParticipantID, WAI_early = WAI.Total)

mid <- apps_alliance |> select(ParticipantID, Session, Early_Session, Mid_Session, WAI.Total) |> 
  # match the alliance score for the specific session that was coded 
  filter(if_else(Mid_Session == 5, Session == 5, Session == 6)) |> 
  select(ParticipantID, WAI_mid = WAI.Total)

# join the isolated alliance scores with the dataset
apps <- left_join(apps, early, by = "ParticipantID") |> 
  left_join(mid, by = "ParticipantID") 

# create average across the two sessions
apps <- apps |> 
  sjmisc::row_means(WAI_early:WAI_mid, n = 0.5, var = "WAI_avg")

rm(early)
rm(mid)
rm(apps_alliance)

full |> 
  select(contains("WAI"))
```

Second, separate out outcome data to separate dataframe & pivot to long format.

```{r separate outcome dataframe}
apps_outcome <- apps |> 
  # select out variables for outcome analyses
  select(ParticipantID, TherapistID, Site, CMHT.BL, CMHT.4M:CMHT.12M, MHgen.BL, MHgen.4M:MHgen.12M, Medication.BL, Medication.4M:Medication.12M, Early_Affirm, Mid_Affirm, Average_Affirm, HAMD_sum.BL:LGBIS_identaffirm_missing, Sx_CASriskacts.BL:SIDAS_sum.12M, WAI_early, WAI_mid, WAI_avg) |> 
  # pivot to long format
    pivot_longer(cols = c(-ParticipantID, -Site, -TherapistID, -Early_Affirm, -Mid_Affirm, -Average_Affirm, -HAMD_missing, -BAI_missing, -RS_missing, -IHS_missing, -SOC_missing, -LGBIS_identaffirm_missing, -WAI_early, -WAI_mid, -WAI_avg),
               names_to = c(".value", "Assessment"),
               names_sep = "\\.") |>
  # clean up assessment variable to be numeric 
  mutate(Assessment = case_when(Assessment == "BL" ~ 0,
                                Assessment == "4M" ~ 1,
                                Assessment == "8M" ~ 2,
                                Assessment == "12M" ~ 3)) |> 
  relocate(Assessment, .after = TherapistID)
```

Thus `apps_outcome` is the data frame for all analyses down below.

# Centering variables

For outcome analyses, we can have assessment be centered around the post-treatment outcome scores. This would mean that the intercept would reflect the mean outcome variable score post-treatment/at 4MFU. The LADI predictor variable would then signify the association b/t affirmative technique and post-treatment outcome scores while accounting for change in outcomes over the 12-month follow-up period.

```{r}
apps_outcome <- mutate(apps_outcome,
                       Assessment_c = Assessment - 1)
```

For all analyses, we will grand-mean center the LGBTQ-APPS variable. We'll also center the average alliance score since we'll include that as a covariate.

```{r}
apps_outcome <- mutate(apps_outcome,
                       Average_Affirm_c = Average_Affirm - mean(Average_Affirm),
                       WAI_avg_c = WAI_avg - mean(WAI_avg),
                       WAI_early_c = WAI_early - mean(WAI_early),
                       WAI_mid_c = WAI_mid - mean(WAI_mid),
                       Sx_totalacts_sum_c = Sx_totalacts_sum - mean(Sx_totalacts_sum, na.rm = T) 
                       )
```

# Create quadratic polynomial term

This will allow us to estimate a non-linear change trajectory, which is pretty common in psychotherapy outcomes. Here, we'll consider just quadratic because cubic is not very tenable with the small sample size that we have - it's likely to overfit to the data.

```{r}
apps_outcome <- mutate(apps_outcome,
                       Assessment_c_quad = Assessment_c * Assessment_c)
```

# Item/scale adjustments

## SIP-AD

For all items, if \<50% of items were missing, responses were imputed with the mean of all non-missing item. For the SIP-AD, this measure is a count (yes/no of whether or not a specific consequence was endorsed). For participant 3352's 12MFU, SIP item 4 was imputed as 0.36. Instead, we modified this value to be the median value among the non-missing responses so we could preserve the dichotomous nature of scale responses for the SIP-AD. For this participant, this median value for the SIP-AD at 12MFU is 0. Thus, this item was changed and 3352's SIP_sum score for the 12MFU is changed from 5.36 to 5 to reflect this change.

```{r}
full <- full |> 
  mutate(SIP_4 = ifelse((ParticipantID == 3352 & Assessment == 3), 0, SIP_4),
         SIP_sum = ifelse((ParticipantID == 3352 & Assessment == 3), 5, SIP_sum))

apps_outcome <- apps_outcome |> 
  mutate(SIP_sum = ifelse((ParticipantID == 3352 & Assessment == 3), 5, SIP_sum))
```

## SIDAS

Given the potential for SIDAS scores to be highly skewed due to low prevalence of suicidality in this sample, we'll also create a variable (0 = no, 1 = yes) as to whether or not suicidality was endorsed at that wave.

```{r}
apps_outcome <- apps_outcome |> 
  mutate(SIDAS_yn = ifelse(SIDAS_sum > 0, 1, 0))  
```

## Missing

We noticed that there were a couple of participants that scored a 999 (SPSS missing value) instead of `NA` (R missing value) on the `SIP_sum` and `Sx_totalacts_sum` measure. This code will recode those values as missing.

```{r}
apps_outcome <- apps_outcome |> 
    mutate(SIP_sum = ifelse(SIP_sum == 999, NA, SIP_sum),
           Sx_totalacts_sum = ifelse(Sx_totalacts_sum == 999, NA, Sx_totalacts_sum))
```

# Covariates

Recode to 0/1 structure for models.

```{r}
apps_outcome <- apps_outcome |> 
  mutate(CMHT = recode_factor(as.factor(CMHT), `0` = "No or no data", `1` = "Continued"),
         MHgen = recode_factor(as.factor(MHgen), `0` = "No or no data", `1` = "Utilized"),
         Medication = recode_factor(as.factor(Medication), `0` = "No or no data", `1` = "Taken"),
         Site = recode_factor(as.factor((Site - 1)), `0` = "NYC", `1` = "Miami")
         )
```

Continued CMHT treatment:

```{r}
# overall
apps_outcome |> frq(CMHT)

# by time point
apps_outcome |>  group_by(Assessment) |>  frq(CMHT)
```

MH treatment elsewhere:

```{r}
# overall
apps_outcome |> frq(MHgen)

# by time point
apps_outcome |>  group_by(Assessment) |>  frq(MHgen)
```

Medication:

```{r}
# overall
apps_outcome |> frq(Medication)

# by time point
apps_outcome |>  group_by(Assessment) |>  frq(Medication)
```

# Descriptives + missing data

Here is code that will pull descriptive information (including % missing data) for all of the variables of interest - this averages across all time points.

```{r}
apps_outcome |>  
  select(Sx_CASriskacts, Sx_totalacts_sum, AUDIT_sum, SIP_sum, HAMD_sum, BAI_sum, SIDAS_sum, SIDAS_yn, IHS_mean, RS_mean, SOC_concealment_mean, LGBIS_identaffirm, Average_Affirm, WAI_avg) |>  
  summarize(across(everything(), list(mean = ~ mean(.x, na.rm = T),
                                      median = ~ median(.x, na.rm = T),
                                      sd = ~ sd(.x, na.rm = T),
                                      min = ~ min(.x, na.rm = T),
                                      max = ~ max(.x, na.rm = T),
                                      na_n = ~ sum(is.na(.x)),
                                      na_perc = ~ (sum(is.na(.x) / n_distinct(apps_outcome))) * 100),
                   .names = "{.col}-{.fn}")
            ) |>  
  pivot_longer(cols = everything(),
               names_to = c("ColNames", ".value"), 
               names_sep = "-",
               ) |>  
  mutate(max = round(max, digits = 2)) |>  
  unite(range, c("min", "max"), sep = "-")
```

Then do this by the specific time point, excluding `Average_Affirm` and `WAI_avg` since these variables did not differ by time point:

```{r}
apps_outcome |>  
  select(Assessment, Sx_CASriskacts, Sx_totalacts_sum, AUDIT_sum, SIP_sum, HAMD_sum, BAI_sum, SIDAS_sum, SIDAS_yn, IHS_mean, RS_mean, SOC_concealment_mean, LGBIS_identaffirm) |>  
  group_by(Assessment) |> 
  summarize(across(everything(), list(mean = ~ mean(.x, na.rm = T),
                                      median = ~ median(.x, na.rm = T),
                                      sd = ~ sd(.x, na.rm = T),
                                      min = ~ min(.x, na.rm = T),
                                      max = ~ max(.x, na.rm = T),
                                      na_n = ~ sum(is.na(.x)),
                                      na_perc = ~ (sum(is.na(.x) / n_distinct(apps_outcome))) * 100),
                   .names = "{.col}-{.fn}")
            ) %>% 
  pivot_longer(cols = -Assessment,
               names_to = c("ColNames", ".value"), 
               names_sep = "-",
               ) %>% 
  mutate(max = round(max, digits = 2),
         min = round(min, digits = 2)) %>% 
  unite(range, c("min", "max"), sep = "-") |> 
  arrange(ColNames)
```

# Scale reliability

For reporting purposes, we're going to pull Cronbach's alpha overall and for each time point. These functions will allow us to do that easily for each time point.

```{r}
#| message: false
#| warning: false

# Define a function to just get the raw Cronbach's alpha from the psych package for each measure (aggregated across time point)
get_raw_alpha <- function(data, measure_cols) {
  alpha <- data %>%
    select({{measure_cols}}) %>%
    alpha(warnings = FALSE)
  
  raw_alpha <- alpha$total$raw_alpha
  return(raw_alpha)
}

# Define a function to calculate Cronbach's alpha for each assessment time
calculate_alpha_map <- function(data, assessment_col, measure_cols) {
  # Get unique assessment times
  assessment_times <- unique(data[[assessment_col]])
  
  # Define a function to calculate alpha for a specific assessment time
  alpha_for_time <- function(assessment_time) {
    data_filtered <- data %>% filter(!!sym(assessment_col) == assessment_time)
    alpha_result <- data_filtered %>% select({{measure_cols}}) %>% alpha(warnings = F)
    return(alpha_result$total$raw_alpha)
  }
  
  # Use map to apply the alpha function to each assessment time
  alpha_results <- map(assessment_times, alpha_for_time)
  names(alpha_results) <- paste("Assessment", assessment_times)
  
  return(alpha_results)
}
```

## AUDIT

```{r}
#| message: false
#| warning: false

get_raw_alpha(full, AUDIT_1:AUDIT_9)
calculate_alpha_map(full, "Assessment", AUDIT_1:AUDIT_9) 
```

## SIP-AD

Note this will give you the KR-20 coefficient when the `alpha` function is run with dichotomous items.

```{r}
#| message: false
#| warning: false

get_raw_alpha(full, SIP_1:SIP_15)
calculate_alpha_map(full, "Assessment", SIP_1:SIP_15) 
```

## HAMD

```{r}
#| message: false
#| warning: false

get_raw_alpha(full, HAMD_1:HAMD_17)
calculate_alpha_map(full, "Assessment", HAMD_1:HAMD_17) 
```

## BAI

```{r}
#| message: false
#| warning: false

get_raw_alpha(full, BAI_1:BAI_21)
calculate_alpha_map(full, "Assessment", BAI_1:BAI_21) 
```

## SIDAS (continuous)

Note for the SIDAS the missing values were not coded as `NA` for item 2 so those need to be recoded.

```{r}
full <- full |> 
    mutate(SIDAS_2r = ifelse(SIDAS_2r == 999, NA, SIDAS_2r))
```

Then calculate reliability:

```{r}
#| message: false
#| warning: false

get_raw_alpha(full, c(SIDAS_1, SIDAS_2r, SIDAS_3, SIDAS_4, SIDAS_5))
calculate_alpha_map(full, "Assessment", c(SIDAS_2r, SIDAS_3, SIDAS_4, SIDAS_5))
```

## IHS

```{r}
#| message: false
#| warning: false

get_raw_alpha(full, IHS_1:IHS_9)
calculate_alpha_map(full, "Assessment", IHS_1:IHS_9) 
```

## RS

```{r}
#| message: false
#| warning: false

get_raw_alpha(full, RS_1:RS_14)
calculate_alpha_map(full, "Assessment", RS_1:RS_14) 
```

## Concealment

```{r}
#| message: false
#| warning: false

get_raw_alpha(full, c(SOC_FamilyR, SOC_GLBFriendsR, SOC_Str8FriendsR, SOC_CoworkersR, SOC_HCPR))
calculate_alpha_map(full, "Assessment", c(SOC_FamilyR, SOC_GLBFriendsR, SOC_Str8FriendsR, SOC_CoworkersR, SOC_HCPR)) 
```

## Identity affirmation

```{r}
#| message: false
#| warning: false

get_raw_alpha(full, c(LGBIS_6, LGBIS_13, LGBIS_26))
calculate_alpha_map(full, "Assessment", c(LGBIS_6, LGBIS_13, LGBIS_26)) 
```

## Working alliance 

Because different sessions were coded, some wrangling is needed to get the item-level alliance data for the session where the LADI was coded. This first chunk will do that and store them in separate dataframes to calculate reliability.

```{r}
### code to pull in the item-by-item data for the alliance
alliance_items <- read_spss("data/WIDE_Therapy Post-Session Surveys for ESTEEM study 2021.10.21.sav")

# create a data frame that denotes which session the LADI was coded for each participant 
apps_alliance_rel <- 
  apps |> 
  select(ParticipantID, Early_Session, Mid_Session)

####### EARLY #######

# select only relevant columns from the alliance data - items 4 & 10 are reverse scored so this code replaces the original with the rescored
early_alliance_items <- alliance_items |> 
  select(ParticipantID, contains("P.2"), contains("P.3"), 
         -WAI4_P.2, -WAI4_P.3, -WAI10_P.2, -WAI10_P.3, 
         WAI4_P_R.2, WAI4_P_R.3, WAI10_P_R.2, WAI10_P_R.3) |> 
  select(ParticipantID, contains("WAI"), 
         WAI4_P_R.2, WAI4_P_R.3, WAI10_P_R.2, WAI10_P_R.3) |> 
  relocate(WAI4_P_R.2, .after = WAI3_P.2) |> 
  relocate(WAI4_P_R.3, .after = WAI3_P.3) |> 
  relocate(WAI10_P_R.2, .after = WAI9_P.2) |> 
  relocate(WAI10_P_R.3, .after = WAI9_P.3) 

# merge the two datasets together
apps_alliance_early <- apps_alliance_rel |> 
  left_join(early_alliance_items, by = "ParticipantID")

# pull the early session 2s
early_s2 <- apps_alliance_early |> 
  filter(Early_Session == 2) |> 
  select(ParticipantID, Early_Session, contains(".2")) |> 
  rename_with(~ str_replace_all(., c("_P.2" = "_early", "_P_R.2" = "_early")))

# pull the early session 3s
early_s3 <- apps_alliance_early |> 
  filter(Early_Session == 3) |> 
  select(ParticipantID, Early_Session, contains(".3")) |> 
  rename_with(~ str_replace_all(., c("_P.3" = "_early", "_P_R.3" = "_early")))

# merge early together
early_alliance <- rbind(early_s2, early_s3)

####### MIDDLE #######

# create a data frame that denotes which session the LADI was coded for each participant 
apps_alliance_rel <- 
  apps |> 
  select(ParticipantID, Early_Session, Mid_Session)

# select only relevant columns from the alliance data - items 4 & 10 are reverse scored so this code replaces the original with the rescored
mid_alliance_items <- alliance_items |> 
  select(ParticipantID, contains("P.5"), contains("P.6"), 
         -WAI4_P.5, -WAI4_P.6, -WAI10_P.5, -WAI10_P.6, 
         WAI4_P_R.5, WAI4_P_R.6, WAI10_P_R.5, WAI10_P_R.6) |> 
  select(ParticipantID, contains("WAI"), 
         WAI4_P_R.5, WAI4_P_R.6, WAI10_P_R.5, WAI10_P_R.6) |> 
  relocate(WAI4_P_R.5, .after = WAI3_P.5) |> 
  relocate(WAI4_P_R.6, .after = WAI3_P.6) |> 
  relocate(WAI10_P_R.5, .after = WAI9_P.5) |> 
  relocate(WAI10_P_R.6, .after = WAI9_P.6) 

# merge the two datasets together
apps_alliance_mid <- apps_alliance_rel |> 
  left_join(mid_alliance_items, by = "ParticipantID")

# pull the mid session 5s
mid_s2 <- apps_alliance_mid |> 
  filter(Mid_Session == 5) |> 
  select(ParticipantID, Mid_Session, contains(".5")) |> 
  rename_with(~ str_replace_all(., c("_P.5" = "_mid", "_P_R.5" = "_mid")))

# pull the mid session 6s
mid_s3 <- apps_alliance_mid |> 
  filter(Mid_Session == 6) |> 
  select(ParticipantID, Mid_Session, contains(".6")) |> 
  rename_with(~ str_replace_all(., c("_P.6" = "_mid", "_P_R.6" = "_mid")))

# merge early together
mid_alliance <- rbind(mid_s2, mid_s3)
```

Reliability for early alliance scores:
```{r}
# calculate reliability for early 
early_alliance |> 
  select(-ParticipantID, -Early_Session) |> 
  alpha()
```

Reliability for middle alliance scores:
```{r}
# calculate reliability for middle 
mid_alliance |> 
  select(-ParticipantID, -Mid_Session) |> 
  alpha()
```

Remove dataframes no longer needed:
```{r}
rm(apps_alliance_early)
rm(apps_alliance_mid)
rm(apps_alliance_rel)
rm(early_alliance)
rm(early_alliance_items)
rm(early_s2)
rm(early_s3)
rm(mid_alliance)
rm(mid_alliance_items)
rm(mid_s2)
rm(mid_s3)
rm(alliance_items)
```

# Overall histograms

```{r}
#| message: false
#| warning: false
 
plot_outcomes_sum <- function(data, outcome_vars) {
  plots <- map(outcome_vars, function(var) {
    var_sym <- sym(var)
    data %>%
      ggplot(aes(x = !!var_sym)) +
      geom_histogram(binwidth = 1)
  })
  
  return(plots)
}

outcome_vars_sum <- c("Sx_CASriskacts", "AUDIT_sum", "SIP_sum", "HAMD_sum", "BAI_sum", "SIDAS_sum", "SIDAS_yn")
plot_outcomes_sum(apps_outcome, outcome_vars_sum)

plot_outcomes_mean <- function(data, outcome_vars) {
  plots <- map(outcome_vars, function(var) {
    var_sym <- sym(var)
    data %>%
      ggplot(aes(x = !!var_sym)) +
      geom_histogram(binwidth = 0.3)
  })
  
  return(plots)
}

outcome_vars_mean <- c("IHS_mean", "RS_mean", "SOC_concealment_mean", "LGBIS_identaffirm")
plot_outcomes_mean(apps_outcome, outcome_vars_mean)
```

# Change trajectories

## Individual plots

```{r}
#| message: false
#| warning: false
 
individ_trajs <- function(data, outcome_vars) {
  plots <- map(outcome_vars, function(var) {
    var_sym <- sym(var)
    data |> 
      ggplot(aes(x = Assessment, y = !!var_sym)) +
      geom_point() +
      stat_smooth(method = "loess", se = FALSE) +
      facet_wrap(~ ParticipantID) 
  })
  
  return(plots)
}

outcome_vars <- c("Sx_CASriskacts", "AUDIT_sum", "SIP_sum", "HAMD_sum", "BAI_sum", "SIDAS_sum", "IHS_mean", "RS_mean", "SOC_concealment_mean")
individ_trajs(apps_outcome, outcome_vars)
```

### Spaghetti plots

```{r}
#| message: false
#| warning: false

spaghetti <- function(data, outcome_vars) {
  plots <- map(outcome_vars, function(var) {
    var_sym <- sym(var)
    data |> 
      ggplot(aes(x = Assessment, y = !!var_sym)) +
      stat_smooth(aes(group = ParticipantID),
              method = "loess", se = F, size = 1/16) +
      stat_smooth(method = "loess", se = F, size = 2, color = "purple")
  })
  
  return(plots)
}

spaghetti(apps_outcome, outcome_vars)
```

# Scatterplots

Overall scores:

```{r}
#| message: false
#| warning: false

#apps_outcome |> 
#  ggplot(aes(x = Average_Affirm, y = HAMD_sum)) + 
#  geom_point() + 
#  geom_jitter() +
#  stat_smooth(method = "loess")

scatter <- function(data, outcome_vars) {
  plots <- map(outcome_vars, function(var) {
    var_sym <- sym(var)
    data |> 
      ggplot(aes(x = Average_Affirm, y = !!var_sym)) +
      geom_point() +
      geom_jitter() +
      stat_smooth(method = "loess")
  })
  
  return(plots)
}

scatter(apps_outcome, outcome_vars)
```

Just post-treatment scores to make this clearer. Note that we wrote over the function above.

```{r}
#| message: false
#| warning: false
 
scatter <- function(data, outcome_vars) {
  plots <- map(outcome_vars, function(var) {
    var_sym <- sym(var)
    data |> 
      filter(Assessment == 1) |> 
      ggplot(aes(x = Average_Affirm, y = !!var_sym)) +
      geom_point() +
      geom_jitter() +
      stat_smooth(method = "loess")
  })
  
  return(plots)
}

scatter(apps_outcome, outcome_vars)
```

Inspection of the bivariate scatterplot shows that there are a couple of points at the far range of the LADI that might be influencing the bivariate relationship. Let's identify what those scores are:

```{r}
apps_outcome |> 
  frq(Average_Affirm)
```

Ok we see here that those are likely the one participant whose therapist scored scored a 4.75 on the LADI. Let's exclude those to see how sensitive the plots are to those two points. Note that we wrote over the function above to make this easier.

```{r}
#| message: false
#| warning: false
 
scatter <- function(data, outcome_vars) {
  plots <- map(outcome_vars, function(var) {
    var_sym <- sym(var)
    data |> 
      filter(Assessment == 1) |> 
      filter(Average_Affirm != 4.75) |> 
      ggplot(aes(x = Average_Affirm, y = !!var_sym)) +
      geom_point() +
      geom_jitter() +
      stat_smooth(method = "loess")
  })
  
  return(plots)
}

scatter(apps_outcome, outcome_vars)
```

By and large the scatterplots are similar with and without the excluded folks. However, the HAMD, IHS, and SOC are all measures that seem to change. Here is the before and after for each to showcase this.

For HAMD:

```{r}
#| message: false
#| warning: false

hamd.1 <- apps_outcome |> 
      filter(Assessment == 1) |> 
      ggplot(aes(x = Average_Affirm, y = HAMD_sum)) +
      geom_point() +
      geom_jitter() +
      stat_smooth(method = "loess")

hamd.2 <- apps_outcome |> 
      filter(Assessment == 1) |> 
      filter(Average_Affirm != 4.75) |> 
      ggplot(aes(x = Average_Affirm, y = HAMD_sum)) +
      geom_point() +
      geom_jitter() +
      stat_smooth(method = "loess")

grid.arrange(hamd.1, hamd.2, ncol = 2)
```

For IHS:

```{r}
#| message: false
#| warning: false

ihs.1 <- apps_outcome |> 
      filter(Assessment == 1) |> 
      ggplot(aes(x = Average_Affirm, y = IHS_mean)) +
      geom_point() +
      geom_jitter() +
      stat_smooth(method = "loess")

ihs.2 <- apps_outcome |> 
      filter(Assessment == 1) |> 
      filter(Average_Affirm != 4.75) |> 
      ggplot(aes(x = Average_Affirm, y = IHS_mean)) +
      geom_point() +
      geom_jitter() +
      stat_smooth(method = "loess")

grid.arrange(ihs.1, ihs.2, ncol = 2)
```

For SOC:

```{r}
#| message: false
#| warning: false

soc.1 <- apps_outcome |> 
      filter(Assessment == 1) |> 
      ggplot(aes(x = Average_Affirm, y = SOC_concealment_mean)) +
      geom_point() +
      geom_jitter() +
      stat_smooth(method = "loess")

soc.2 <- apps_outcome |> 
      filter(Assessment == 1) |> 
      filter(Average_Affirm != 4.75) |> 
      ggplot(aes(x = Average_Affirm, y = SOC_concealment_mean)) +
      geom_point() +
      geom_jitter() +
      stat_smooth(method = "loess")

grid.arrange(soc.1, soc.2, ncol = 2)
```

# Correlations

We'll pull a correlation matrix for variables across all time points.

```{r}
corr_results <- correlation(data = apps_outcome, 
            select = c("Sx_CASriskacts", "Sx_totalacts_sum", "AUDIT_sum", "SIP_sum", "HAMD_sum", "BAI_sum", "SIDAS_sum", "IHS_mean", "RS_mean", "SOC_concealment_mean", "Average_Affirm", "WAI_avg")) |> 
  summary(redundant = T) 
corr_results
```

# ICCs

We'll calculate ICCs from a random intercepts MLM. First, we'll evaluate nesting of timepoints within participants.

```{r}
# function to calculate ICCs for all outcomes
icc_pt <- apps_outcome %>% 
  select(ParticipantID, Assessment_c, Sx_CASriskacts, Sx_totalacts_sum, AUDIT_sum, SIP_sum, HAMD_sum, BAI_sum, SIDAS_sum, IHS_mean, RS_mean, SOC_concealment_mean, LGBIS_identaffirm) %>% 
  pivot_longer(c(Sx_CASriskacts, Sx_totalacts_sum, AUDIT_sum, SIP_sum, HAMD_sum, BAI_sum, SIDAS_sum, IHS_mean, RS_mean, SOC_concealment_mean, LGBIS_identaffirm),
               names_to = "variable",
               values_to = "outcome") %>% 
  mutate(variable = as.factor(variable)) %>% 
  group_by(variable) %>% 
  group_split() %>% 
  map_dfr(.f = function(df) {
    lmer(outcome ~ 1 + (1 | ParticipantID), REML = T, data = df) %>% 
      broom.mixed::tidy(effects = "ran_pars", ddf.method = "Kenward-Roger", scales = "vcov") %>% 
      add_column(variable = unique(df$variable), .before = 1) %>% 
      group_by(variable) %>%
      summarize(pt_var = first(estimate),
                total_var = sum(estimate),
                icc = pt_var/total_var)
  })
icc_pt %>% select(variable, icc) %>% mutate_if(is.numeric, round, digits = 3)
```

Second, we'll consider nesting of participants within therapists.

```{r}
icc_ther <- apps_outcome |>  
  select(ParticipantID, TherapistID, Sx_CASriskacts, Sx_totalacts_sum, AUDIT_sum, SIP_sum, HAMD_sum, BAI_sum, SIDAS_sum, IHS_mean, RS_mean, SOC_concealment_mean, LGBIS_identaffirm) |>  
  pivot_longer(c(Sx_CASriskacts, Sx_totalacts_sum, AUDIT_sum, SIP_sum, HAMD_sum, BAI_sum, SIDAS_sum, IHS_mean, RS_mean, SOC_concealment_mean, LGBIS_identaffirm),
               names_to = "variable",
               values_to = "outcome") |> 
  mutate(variable = as.factor(variable)) |>  
  group_by(variable) |> 
  group_split() |>  
  map_dfr(.f = function(df) {
    lmer(outcome ~ 1 + (1 | ParticipantID) + (1 | TherapistID), REML = T, data = df) |>  
      broom.mixed::tidy(effects = "ran_pars", ddf.method = "Kenward-Roger", scales = "vcov") |>  
      add_column(variable = unique(df$variable), .before = 1) |>  
      group_by(variable) |>  
      summarize(pt_var = first(estimate),
                ther_var = nth(estimate, 2),
                total_var = sum(estimate),
                icc_ther = ther_var / total_var,
                icc_pt = (pt_var + ther_var) / total_var)
  })
icc_ther |> select(variable, icc_pt, icc_ther) |> mutate_if(is.numeric, round, digits = 3)
```

Of note, there were 24 unique therapists and many only had 1 participant in the sample.

```{r}
apps_outcome |> 
  group_by(TherapistID) |> 
  summarize(n_distinct(ParticipantID)) |> 
  rename(number_pts = "n_distinct(ParticipantID)")
```

# Demographics

The demographics that were reported in the primary outcome paper include: age, race, ethnicity, sex assigned at birth, gender identity, sexual orientation, education (coded as less than college vs college degree), employment status, past-year personal income (coded as up to \$39k, 30-75k or more), and relationship status.

Let's pull those variables:

```{r}
demos <- full |> 
  filter(Assessment == 0) |> 
  select(DQ_age, # age
       DQ_raceEthnicity, # race
       DQ_raceEthnicity.text, # race free response 
       DQ_latinx, # ethnicity 
       DQ_birth.sex, # assigned sex at birth
       DQ_GenID.man:DQ_GenID.other, # gender identity
       DQ_SexualOrientation, # sexual orientation
       DQ_education, # education
       DQ_employment, # employment status
       DQ_income, # personal income past year
       DQ_relationshipStatus# relationship status 
       )
```

Then recode relevant variables to be consistent with main protocol paper reporting:

```{r}
# sexual orientation
demos <- demos |> mutate(DQ_SOr = case_when(DQ_SexualOrientation == 1 ~ 1,
                                   DQ_SexualOrientation == 2 ~ 2,
                                   DQ_SexualOrientation == 3 ~ 2,
                                   DQ_SexualOrientation == 4 ~ 2,
                                   DQ_SexualOrientation == 6 ~ 3,
                                   DQ_SexualOrientation == 7 ~ 4),
                DQ_SOr = recode_factor(DQ_SOr, `1` = "Gay", `2` = "Bi", `3` = "Queer", `4` = "Uncertain")) |> 
  # education
  mutate(DQ_educr = case_when(DQ_education == 1 ~ 1,
                              DQ_education == 2 ~ 1, 
                             .default = 2),
       DQ_educr = recode_factor(DQ_educr, `1` = "Less than college", `2` = "College degree")) |> 
  # income
  mutate(DQ_incr = case_when(DQ_income == 1 ~ 1,
                             DQ_income == 2 ~ 1,
                             DQ_income == 3 ~ 1,
                             .default = 2),
         DQ_incr = recode_factor(DQ_incr, `1` = "up to $29k", `2` = "$30-75k or more")) |> 
  # relationship status
  mutate(DQ_relstatusr = case_when(DQ_relationshipStatus == 4 ~ 1,
                                   DQ_relationshipStatus == 5 ~ 2,
                                   .default = 3),
         DQ_relstatusr = recode_factor(DQ_relstatusr, `1` = "casually dating", `2` = "single", `3` = "in a relationship")) 
```

Pull demographic information for reporting:

```{r}
demos |> 
  select(DQ_age, DQ_raceEthnicity, DQ_latinx, DQ_birth.sex, DQ_GenID.man:DQ_GenID.other, DQ_SOr, DQ_educr, DQ_employment, DQ_incr, DQ_relstatusr) |> 
  frq()
```

# MINI diagnoses

```{r}
full |>
  select(ParticipantID, Assessment, contains("MINI")) |> 
  filter(Assessment == 0) |> 
  select(MINI_Depression, MINI_Dysthymia, MINI_PanicDisorder, MINI_Agoraphobia, MINI_SocialAnxiety, MINI_OCD, MINI_PTSD, MINI_GAD, MINI_AUD, MINI_SUD) |> 
    frq()
```


# Save data file

```{r}
saveRDS(apps_outcome, file = "data/apps_outcome.rds")
```

# Output

## Correlation matrix

```{r}
# computes correlation matrix in a dataframe that can be saved and easily reformated for publication
corr_results <- correlation(data = apps_outcome, select = c("Sx_CASriskacts", "AUDIT_sum", "SIP_sum", "HAMD_sum", "BAI_sum", "SIDAS_sum", "IHS_mean", "RS_mean", "SOC_concealment_mean", "LGBIS_identaffirm", "Average_Affirm")) |> 
  as.data.frame() |> 
  select(Parameter1, Parameter2, r, p) |> 
  mutate(r = sprintf("%.2f", r), # keeps the trailing zeros for correlations
         ps = ifelse(p < .10, '+', ''),
         ps = ifelse(p < .05, '*', ps),
         ps = ifelse(p < .01, '**', ps),
         ps = ifelse(p < .001, '***', ps),
         r = as.character(r)) |> 
  tidyr::unite(r_p, c("r", "ps"), sep = "") |> 
  select(-p)

write.csv(corr_results, "output/corr_table.csv")

corr_results
```

## Descriptives

Pull the descriptives for main study variables for reporting:

```{r}
descriptives_results <- apps_outcome |>  
  select(Sx_CASriskacts, AUDIT_sum, SIP_sum, HAMD_sum, BAI_sum, SIDAS_sum, IHS_mean, RS_mean, SOC_concealment_mean, LGBIS_identaffirm, Average_Affirm) |>  
  summarize(across(everything(), list(mean = ~ mean(.x, na.rm = T),
                                      median = ~ median(.x, na.rm = T),
                                      sd = ~ sd(.x, na.rm = T),
                                      min = ~ min(.x, na.rm = T),
                                      max = ~ max(.x, na.rm = T)),
                   .names = "{.col}-{.fn}")
            ) |>  
  pivot_longer(cols = everything(),
               names_to = c("ColNames", ".value"), 
               names_sep = "-",
               ) |> 
  mutate_if(is.numeric, round, digits = 2) |>
  mutate(sd = paste0('(', sd) %>% paste0(')')) |> 
  tidyr::unite(mean_sd, c("mean", "sd"), sep = " ") |> 
  tidyr::unite(range, c("min", "max"), sep = "-") |> 
  select(-median)

write.csv(descriptives_results, "output/descriptives.csv")

descriptives_results
```

# Session info

```{r}
sessionInfo()
```

